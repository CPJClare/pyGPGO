@misc{Shahriari2016,
abstract = {—Big data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., rec-ommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involves many tunable config-uration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and {De Freitas}, Nando},
booktitle = {Proceedings of the IEEE},
doi = {10.1109/JPROC.2015.2494218},
eprint = {arXiv:1011.1669v3},
isbn = {0018-9219},
issn = {00189219},
keywords = {decision making,design of experiments,genomic medicine,optimization,response surface methodology,statistical learning},
number = {1},
pages = {148--175},
pmid = {25246403},
title = {{Taking the human out of the loop: A review of Bayesian optimization}},
volume = {104},
year = {2016}
}

@article{Brochu2010,
abstract = {We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments---active user modelling with preferences, and hierarchical reinforcement learning---and a discussion of the pros and cons of Bayesian optimization based on our experiences.},
archivePrefix = {arXiv},
arxivId = {1012.2599},
author = {Brochu, E and Cora, V M and {De Freitas}, N},
doi = {1012.2599},
eprint = {1012.2599},
journal = {ArXiv},
pages = {49},
title = {{A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning}},
year = {2010}
}


@article{Bergstra2012,
abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimiza-tion. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a compar-ison with a large previous study that used grid search and manual search to configure neural net-works and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising con-figuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent " High Throughput " methods achieve surprising success—they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural base-line against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
author = {{Bergstra}, James and {Yoshua Bengio}},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,global optimization,model selection,neural networks,response surface modeling},
pages = {281--305},
title = {{Random Search for Hyper-Parameter Optimization}},
volume = {13},
year = {2012}
}

@article{Jones2001,
abstract = {This paper presents a taxonomy of existing approaches for using response surfaces for global optimization. Each method is illustrated with a simple numerical example that brings out its advantages and disadvantages. The central theme is that methods that seem quite reasonable often have non-obvious failure modes. Understanding these failure modes is essential for the development of practical algorithms that fulfill the intuitive promise of the response surface approach.},
author = {Jones, D. R.},
doi = {10.1023/A:1012771025575},
isbn = {0925-5001},
issn = {09255001},
journal = {Journal of Global Optimization},
keywords = {global optimization,kriging,response surface,splines},
pages = {345--383},
title = {{A Taxonomy of Global Optimization Methods Based on Response Surfaces}},
url = {http://www.ingentaconnect.com/content/klu/jogo/2001/00000021/00000004/00360694},
volume = {21},
year = {2001}
}

@article{Bull2011,
abstract = {In the efficient global optimization problem, we minimize an unknown function f, using as fewobservations f(x) as possible. It can be considered a continuum-armed-bandit problem, with noiseless data, and simple regret. Expected-improvement algorithms are perhaps the most popular methods for solving the problem; in this paper, we provide theoretical results on their asymptotic behaviour. Implementing these algorithms requires a choice of Gaussian-process prior, which determines an associated space of functions, its reproducing-kernel Hilbert space (RKHS). When the prior is fixed, expected improvement is known to converge on the minimum of any function in its RKHS. We provide convergence rates for this procedure, optimal for functions of low smoothness, and describe a modified algorithm attaining optimal rates for smoother functions. In practice, however, priors are typically estimated sequentially from the data. For standard estimators, we show this procedure may never find the minimum of f. We then propose alternative estimators, chosen to minimize the constants in the rate of convergence, and show these estimators retain the convergence rates of a fixed prior.$\backslash$n},
archivePrefix = {arXiv},
arxivId = {arXiv:1101.3501v3},
author = {Bull, Adam D},
eprint = {arXiv:1101.3501v3},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {Bayesian optimization,convergence rate,expected improvement,global optimization},
pages = {2879--2904},
title = {{Convergence Rates of Efficient Global Optimization Algorithms}},
volume = {12},
year = {2011}
}

@article{Snoek2012,
abstract = {The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a {\{}$\backslash$textquoteleft{\}}{\{}$\backslash$textquoteleft{\}}black art{\{}$\backslash$textquoteright{\}}{\{}$\backslash$textquoteright{\}} requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm{\{}$\backslash$textquoteright{\}}s generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expert-level performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1206.2944v2},
author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
doi = {2012arXiv1206.2944S},
eprint = {arXiv:1206.2944v2},
isbn = {9781627480031},
issn = {10495258},
journal = {Adv. Neural Inf. Process. Syst. 25},
keywords = {bayesian optimization,deep learning,gaussian process},
pages = {1--9},
pmid = {9377276},
title = {{Practical Bayesian Optimization of Machine Learning Algorithms}},
year = {2012}
}

@article{Lai1985,
abstract = {The authors consider multiarmed bandit problems with switching cost, define uniformly good allocation rules, and restrict attention to such rules. They present a lower bound on the asymptotic performance of uniformly good allocation rules and construct an allocation scheme that achieves the bound. It is found that despite the inclusion of a switching cost the proposed allocation scheme achieves the same asymptotic performance as the optimal rule for the bandit problem without switching cost. This is made possible by grouping together samples into blocks of increasing sizes, thereby reducing the number of switches to O(log {\textless}e1{\textgreater}n{\textless}/e1{\textgreater}). Finally, an optimal allocation scheme for a large class of distributions which includes members of the exponential family is illustrated},
author = {Lai, T. L. and Robbins, Herbert},
doi = {10.1016/0196-8858(85)90002-8},
isbn = {0196-8858},
issn = {10902074},
journal = {Advances in Applied Mathematics},
number = {1},
pages = {4--22},
title = {{Asymptotically efficient adaptive allocation rules}},
volume = {6},
year = {1985}
}

@article{Srinivas2010,
abstract = {Many applications require optimizing an unknown, noisy function that is expensive to evaluate. We formalize this task as a multi-armed bandit problem, where the payoff function is either sampled from a Gaussian process (GP) or has low RKHS norm. We resolve the important open problem of deriving regret bounds for this setting, which imply novel convergence rates for GP optimization. We analyze GP-UCB, an intuitive upper-confidence based algorithm, and bound its cumulative regret in terms of maximal information gain, establishing a novel connection between GP optimization and experimental design. Moreover, by bounding the latter in terms of operator spectra, we obtain explicit sublinear regret bounds for many commonly used covariance functions. In some important cases, our bounds have surprisingly weak dependence on the dimensionality. In our experiments on real sensor data, GP-UCB compares favorably with other heuristical GP optimization approaches.},
archivePrefix = {arXiv},
arxivId = {0912.3995},
author = {Srinivas, Niranjan and Krause, Andreas and Kakade, Sham M. and Seeger, Matthias},
doi = {10.1109/TIT.2011.2182033},
eprint = {0912.3995},
isbn = {9781605589077},
issn = {00189448},
journal = {Proceedings of the 27th International Conference on Machine Learning (ICML 2010)},
keywords = {Gaussian Process Bandits, Experimental Design, Ban},
pages = {1015--1022},
title = {{Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design}},
url = {http://arxiv.org/abs/0912.3995},
year = {2010}
}

@inproceedings{Kaufmann2012,
abstract = {The question of the optimality of Thompson Sampling for solving the stochastic multi-armed bandit problem had been open since 1933. In this paper we answer it positively for the case of Bernoulli rewards by providing the first finite-time analysis that matches the asymptotic rate given in the Lai and Robbins lower bound for the cumulative regret. The proof is accompanied by a numerical comparison with other optimal policies, experiments that have been lacking in the literature until now for the Bernoulli case.},
archivePrefix = {arXiv},
arxivId = {arXiv:1205.4217v2},
author = {Kaufmann, Emilie and Korda, Nathaniel and Munos, R{\'{e}}mi},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-34106-9_18},
eprint = {arXiv:1205.4217v2},
isbn = {9783642341052},
issn = {03029743},
pages = {199--213},
title = {{Thompson sampling: An asymptotically optimal finite-time analysis}},
volume = {7568 LNAI},
year = {2012}
}

@article{Rahimi2007,
abstract = {To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. Our randomized features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user specified shift-invariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classification and regression tasks linear machine learning algorithms that use these features outperform state-of-the-art large-scale kernel machines.},
author = {Rahimi, Ali and Recht, Ben},
doi = {10.1.1.145.8736},
isbn = {160560352X},
issn = {0033-6599},
journal = {Advances in neural information {\ldots}},
number = {1},
pages = {1--8},
pmid = {4519940},
title = {{Random features for large-scale kernel machines}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2007{\_}833.pdf},
year = {2007}
}

@article{Hernandez-Lobato2014,
abstract = { We propose a novel information-theoretic approach$\backslash$nfor Bayesian optimization called Predictive Entropy$\backslash$nSearch (PES). At each iteration, PES selects the$\backslash$nnext evaluation point that maximizes the expected$\backslash$ninformation gained with respect to the global$\backslash$nmaximum. PES codifies this intractable acquisition$\backslash$nfunction in terms of the expected reduction in the$\backslash$ndifferential entropy of the predictive distribution.$\backslash$nThis reformulation allows PES to obtain$\backslash$napproximations that are both more accurate and$\backslash$nefficient than other alternatives such as Entropy$\backslash$nSearch (ES). Furthermore, PES can easily perform a$\backslash$nfully Bayesian treatment of the model$\backslash$nhyperparameters while ES cannot. We evaluate PES in$\backslash$nboth synthetic and realworld applications, including$\backslash$noptimization problems in machine learning, finance,$\backslash$nbiotechnology, and robotics. We show that the$\backslash$nincreased accuracy of PES leads to significant gains$\backslash$nin optimization performance. },
archivePrefix = {arXiv},
arxivId = {arXiv:1406.2541v1},
author = {Hern{\'{a}}ndez-Lobato, Jos{\'{e}} Miguel and Hoffman, Matthew W and Ghahramani, Zoubin},
eprint = {arXiv:1406.2541v1},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 28},
pages = {1--9},
title = {{Predictive Entropy Search for Efficient Global Optimization of Black-box Functions}},
url = {https://jmhldotorg.files.wordpress.com/2014/10/pes-final.pdf},
year = {2014}
}

@article{Villemonteix2009,
abstract = {In many global optimization problems motivated by engineering applications, the number of function evaluations is severely limited by time or cost. To ensure that each evaluation contributes to the localization of good candidates for the role of global minimizer, a sequential choice of evaluation points is usually carried out. In particular, when Kriging is used to interpolate past evaluations, the uncertainty associated with the lack of information on the function can be expressed and used to compute a number of criteria accounting for the interest of an additional evaluation at any given point. This paper introduces minimizer entropy as a new Kriging-based criterion for the sequential choice of points at which the function should be evaluated. Based on $\backslash$emph{\{}stepwise uncertainty reduction{\}}, it accounts for the informational gain on the minimizer expected from a new evaluation. The criterion is approximated using conditional simulations of the Gaussian process model behind Kriging, and then inserted into an algorithm similar in spirit to the $\backslash$emph{\{}Efficient Global Optimization{\}} (EGO) algorithm. An empirical comparison is carried out between our criterion and $\backslash$emph{\{}expected improvement{\}}, one of the reference criteria in the literature. Experimental results indicate major evaluation savings over EGO. Finally, the method, which we call IAGO (for Informational Approach to Global Optimization) is extended to robust optimization problems, where both the factors to be tuned and the function evaluations are corrupted by noise.},
archivePrefix = {arXiv},
arxivId = {cs/0611143},
author = {Villemonteix, Julien and Vazquez, Emmanuel and Walter, Eric},
doi = {10.1007/s10898-008-9354-2},
eprint = {0611143},
isbn = {0925-5001},
issn = {09255001},
journal = {Journal of Global Optimization},
keywords = {Gaussian process,Global optimization,Kriging,Robust optimization,Stepwise uncertainty reduction},
number = {4},
pages = {509--534},
primaryClass = {cs},
title = {{An informational approach to the global optimization of expensive-to-evaluate functions}},
volume = {44},
year = {2009}
}

@article{Hennig2012,
abstract = {Contemporary global optimization algorithms are based on local measures of utility, rather than a probability measure over location and value of the optimum. They thus attempt to collect low function values, not to learn about the optimum. The reason for the absence of probabilistic global optimizers is that the corresponding inference problem is intractable in several ways. This paper develops desiderata for probabilistic optimization algorithms, then presents a concrete algorithm which addresses each of the computational intractabilities with a sequence of approximations and explicitly addresses the decision problem of maximizing information gain from each evaluation.},
archivePrefix = {arXiv},
arxivId = {1112.1217},
author = {Hennig, Phillipp and Schuler, Christian J},
doi = {http://dx.doi.org/10.1063/1.1699114},
eprint = {1112.1217},
isbn = {1532-4435},
issn = {15324435},
journal = {Machine Learning Research},
keywords = {expectation propagation,gaussian processes,information,optimization,probability},
number = {1999},
pages = {1809--1837},
pmid = {2797},
title = {{Entropy Search for Information-Efficient Global Optimization}},
volume = {13},
year = {2012}
}

@article{Hoffman2011,
abstract = {Bayesian optimization with Gaussian pro- cesses has become an increasingly popular tool in the machine learning community. It is efficient and can be used when very little is known about the objective function, mak- ing it popular in expensive black-box optimization scenarios. It uses Bayesian methods to sample the objective efficiently using an acquisition function which incorporates the posterior estimate of the objective. However, there are several different parameterized acquisition functions in the literature, and it is often unclear which one to use. Instead of using a single acquisition function, we adopt a portfolio of acquisition functions governed by an online multi-armed bandit strategy. We propose several portfolio strategies, the best of which we call GP-Hedge, and show that this method outperforms the best individ- ual acquisition function. We also provide a theoretical bound on the algorithm's performance.},
archivePrefix = {arXiv},
arxivId = {arXiv:1009.5419v1},
author = {Hoffman, Matthew and Brochu, Eric and Freitas, Nando De},
eprint = {arXiv:1009.5419v1},
isbn = {978-0-9749039-7-2},
journal = {Conference on Uncertainty in Artificial Intelligence},
pages = {327--336},
title = {{Portfolio Allocation for Bayesian Optimization}},
year = {2011}
}

@inproceedings{Auer1995,
abstract = {In the multi-armed bandit problem, a gambler must decide which arm$\backslash$nof K non-identical slot machines to play in a sequence of trials so as$\backslash$nto maximize his reward. This classical problem has received much$\backslash$nattention because of the simple model it provides of the trade-off$\backslash$nbetween exploration (trying out each arm to find the best one) and$\backslash$nexploitation (playing the arm believed to give the best payoff). Past$\backslash$nsolutions for the bandit problem have almost always relied on$\backslash$nassumptions about the statistics of the slot machines. In this work, we$\backslash$nmake no statistical assumptions whatsoever about the nature of the$\backslash$nprocess generating the payoffs of the slot machines. We give a solution$\backslash$nto the bandit problem in which an adversary, rather than a well-behaved$\backslash$nstochastic process, has complete control over the payoffs. In a sequence$\backslash$nof T plays, we prove that the expected per-round payoff of our algorithm$\backslash$napproaches that of the best arm at the rate O(T-1/3), and we$\backslash$ngive an improved rate of convergence when the best arm has fairly low$\backslash$npayoff. We also consider a setting in which the player has a team of$\backslash$n{\&}ldquo;experts{\&}rdquo; advising him on which arm to play; here, we give a$\backslash$nstrategy that will guarantee expected payoff close to that of the best$\backslash$nexpert. Finally, we apply our result to the problem of learning to play$\backslash$nan unknown repeated matrix game against an all-powerful adversary},
author = {Auer, P. and Cesa-Bianchi, N. and Freund, Y. and Schapire, R.E.},
booktitle = {Proceedings of IEEE 36th Annual Foundations of Computer Science},
doi = {10.1109/SFCS.1995.492488},
isbn = {0-8186-7183-1},
issn = {0272-5428},
number = {68},
pages = {322--331},
title = {{Gambling in a rigged casino: The adversarial multi-armed bandit problem}},
volume = {68},
year = {1995}
}

@article{Shahriari2014,
abstract = {Bayesian optimization is a sample-efficient method for black-box global optimization. How- ever, the performance of a Bayesian optimization method very much depends on its exploration strategy, i.e. the choice of acquisition function, and it is not clear a priori which choice will result in superior performance. While portfolio methods provide an effective, principled way of combining a collection of acquisition functions, they are often based on measures of past performance which can be misleading. To address this issue, we introduce the Entropy Search Portfolio (ESP): a novel approach to portfolio construction which is motivated by information theoretic considerations. We show that ESP outperforms existing portfolio methods on several real and synthetic problems, including geostatistical datasets and simulated control tasks. We not only show that ESP is able to offer performance as good as the best, but unknown, acquisition function, but surprisingly it often gives better performance. Finally, over a wide range of conditions we find that ESP is robust to the inclusion of poor acquisition functions.},
archivePrefix = {arXiv},
arxivId = {1406.4625},
author = {Shahriari, Bobak and Wang, Ziyu and Hoffman, Matthew W. and Bouchard-C{\^{o}}t{\'{e}}, Alexandre and de Freitas, Nando},
eprint = {1406.4625},
journal = {arXiv preprint arXiv:1406.4625},
pages = {10},
title = {{An Entropy Search Portfolio for Bayesian Optimization}},
url = {http://arxiv.org/abs/1406.4625},
year = {2014}
}

@article{Neal2011,
abstract = {Hamiltonian dynamics can be used to produce distant proposals for the Metropolis algorithm, thereby avoiding the slow exploration of the state space that results from the diffusive behaviour of simple random-walk proposals. Though originating in physics, Hamiltonian dynamics can be applied to most problems with continuous state spaces by simply introducing fictitious "momentum" variables. A key to its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories can thus be used to define complex mappings without the need to account for a hard to compute Jacobian factor - a property that can be exactly maintained even when the dynamics is approximated by discretizing time. In this review, discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present some of its variations, including using windows of states for approximations, tempering during the course of a trajectory to handle isolated modes, and short-cut methods that prevent useless trajectories form taking much computation time.},
archivePrefix = {arXiv},
arxivId = {1206.1901},
author = {Neal, Radford M.},
doi = {doi:10.1201/b10905-6},
eprint = {1206.1901},
isbn = {9781420079418},
issn = {{\textless}null{\textgreater}},
journal = {Handbook of Markov Chain Monte Carlo},
keywords = {hamiltonian dynamics,mcmc},
pages = {113--162},
pmid = {25246403},
title = {{MCMC using Hamiltonian dynamics}},
year = {2011}
}

@inproceedings{Benassi2011,
abstract = {We consider the problem of optimizing a real-valued continuous function f, which is supposed to be expensive to evaluate and, consequently, can only be evaluated a limited number of times. This article focuses on the Bayesian approach to this},
author = {Benassi, Romain and Bect, Julien and Vazquez, Emmanuel},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-25566-3_13},
isbn = {9783642255656},
issn = {03029743},
pages = {176--190},
title = {{Robust Gaussian process-based global optimization using a fully Bayesian expected improvement criterion}},
volume = {6683 LNCS},
year = {2011}
}

@article{Vazquez2010,
abstract = {This paper deals with the convergence of the expected improvement algorithm, a popular global optimization algorithm based on a Gaussian process model of the function to be optimized. The first result is that under some mild hypotheses on the covariance function k of the Gaussian process, the expected improvement algorithm produces a dense sequence of evaluation points in the search domain, when the function to be optimized is in the reproducing kernel Hilbert space generated by k. The second result states that the density property also holds for P-almost all continuous functions, where P is the (prior) probability distribution induced by the Gaussian process. ?? 2010 Elsevier B.V.},
archivePrefix = {arXiv},
arxivId = {0712.3744},
author = {Vazquez, Emmanuel and Bect, Julien},
doi = {10.1016/j.jspi.2010.04.018},
eprint = {0712.3744},
isbn = {0378-3758},
issn = {03783758},
journal = {Journal of Statistical Planning and Inference},
keywords = {Bayesian optimization,Computer experiments,Gaussian process,Global optimization,RKHS,Sequential esign},
number = {11},
pages = {3088--3095},
title = {{Convergence properties of the expected improvement algorithm with fixed mean and covariance functions}},
volume = {140},
year = {2010}
}

@article{Bardenet2010,
abstract = {In global optimization, when the evaluation of the target function is costly, the usual strategy is to learn a surrogate model for the target function and replace the initial opti-mization by the optimization of the model. Gaussian processes have been widely used since they provide an elegant way to model the fitness and to deal with the exploration-exploitation trade-off in a principled way. Several empirical criteria have been proposed to drive the model optimization, among which is the well-known Expected Improve-ment criterion. The major computational bottleneck of these algorithms is the exhaus-tive grid search used to optimize the highly multimodal merit function. In this paper, we propose a competitive " adaptive grid " ap-proach, based on a properly derived cross-entropy optimization algorithm with mix-ture proposals. Experiments suggest that 1) we outperform the classical single-Gaussian cross-entropy method when the fitness func-tion is highly multimodal, and 2) we improve on standard exhaustive search in GP-based surrogate optimization.},
author = {Bardenet, R{\'{e}}mi and K{\'{e}}gl, Bal{\'{a}}zs},
isbn = {9781605589077},
journal = {Proceedings of the 27th International Conference on Machine Learning},
keywords = {active learning,cross-entropy method,gaussian processes,optimization,surrogate models for global},
pages = {55--62},
title = {{Surrogating the surrogate: accelerating Gaussian-process-based global optimization with a mixture cross-entropy algorithm}},
year = {2010}
}

@article{Lizotte2012,
abstract = {Response surface methods, and global optimization techniques in general, are typically evaluated using a small number of standard synthetic test problems, in the hope that these are a good surrogate for real-world problems. We introduce a new, more rigor- ous methodology for evaluating global optimization techniques that is based on generating thousands of test functions and then evaluating algorithm performance on each one. The test functions are generated by sampling from a Gaussian process, which allows us to create a set of test functions that are interesting and diverse. They will have different numbers of modes, different maxima, etc., and yet they will be similar to each other in overall structure and level of difficulty. This approach allows for a much richer empirical evaluation of methods that is capable of revealing insights that would not be gained using a small set of test functions. To facilitate the development of large empirical studies for evaluating response surface methods, we introduce a dimension-independent measure of average test problem difficulty, and we introduce acquisition criteria that are invariant to vertical shifting and scaling of the objective function. We also use our experimental methodology to conduct a large empirical study of response surface methods.We investigate the influence of three properties—parameter esti- mation, exploration level, and gradient information—on the performance of response surface methods.},
author = {Lizotte, Daniel James and Greiner, Russell and Schuurmans, Dale},
doi = {10.1007/s10898-011-9732-z},
issn = {09255001},
journal = {Journal of Global Optimization},
keywords = {Global optimization,Response surface,Surrogate model},
number = {4},
pages = {699--736},
title = {{An experimental methodology for response surface optimization methods}},
volume = {53},
year = {2012}
}

@article{Hansen2001,
abstract = {This paper puts forward two useful methods for self-adaptation of the mutation distribution - the concepts of derandomization and cumulation. Principle shortcomings of the concept of mutative strategy parameter control and two levels of derandomization are reviewed. Basic demands on the self-adaptation of arbitrary (normal) mutation distributions are developed. Applying arbitrary, normal mutation distributions is equivalent to applying a general, linear problem encoding. The underlying objective of mutative strategy parameter control is roughly to favor previously selected mutation steps in the future. If this objective is pursued rigorously, a completely derandomized self-adaptation scheme results, which adapts arbitrary normal mutation distributions. This scheme, called covariance matrix adaptation (CMA), meets the previously stated demands. It can still be considerably improved by cumulation - utilizing an evolution path rather than single search steps. Simulations on various test functions reveal local and global search properties of the evolution strategy with and without covariance matrix adaptation. Their performances are comparable only on perfectly scaled functions. On badly scaled, non-separable functions usually a speed up factor of several orders of magnitude is observed. On moderately mis-scaled functions a speed up factor of three to ten can be expected.},
author = {Hansen, Nikolaus and Ostermeier, Andreas},
doi = {10.1162/106365601750190398},
isbn = {1063-6560},
issn = {1063-6560},
journal = {Evolutionary Computation},
keywords = {covariance matrix adaptation,cumulation,cumulative path length control,de-,derandomized self-adaptation,evolu-,evolution strategy,randomization,self-adaptation,step size control,strategy parameter control,tion path},
number = {2},
pages = {159--195},
pmid = {11382355},
title = {{Completely Derandomized Self-Adaptation in Evolution Strategies}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/106365601750190398},
volume = {9},
year = {2001}
}

@article{Kocsis2006,
abstract = {For large state-space Markovian Decision Problems Monte-Carlo planning is one of the few viable approaches to find near-optimal solutions. In this paper we introduce a new algorithm, UCT, that applies bandit ideas to guide Monte-Carlo planning. In finite-horizon or discounted MDPs the algorithm is shown to be consistent and finite sample bounds are derived on the estimation error due to sampling. Experimental results show that in several domains, UCT is significantly more efficient than its alternatives.},
author = {Kocsis, Levente and Szepesv{\'{a}}ri, Csaba},
doi = {10.1007/11871842},
isbn = {978-3-540-45375-8},
issn = {03029743},
journal = {Proceedings of ECML},
pages = {282--203},
title = {{Bandit based monte-carlo planning}},
url = {http://link.springer.com/chapter/10.1007/11871842{\_}29},
year = {2006}
}

@article{Bubeck2010,
abstract = {We consider a generalization of stochastic bandits where the set of arms, X, is allowed to be a generic measurable space and the mean-payoff function is “locally Lipschitz” with respect to a dissimilarity function that is known to the decision maker. Under this condition we construct an arm selection policy, called HOO (hierarchical optimistic optimization), with improved regret bounds compared to previous results for a large class of problems. In particular, our results imply that if X is the unit hypercube in a Euclidean space and the mean-payoff function has a finite number of global maxima around which the behavior of the function is locally continuous with a known smoothness degree, then the expected regret of HOO is bounded up to a logarithmic factor by √n, i.e., the rate of growth of the regret is independent of the dimension of the space. We also prove the minimax optimality of our algorithm when the dissimilarity is a metric. Our basic strategy has quadratic computational complexity as a function of the number of time steps and does not rely on the doubling trick. We also introduce a modified strategy, which relies on the doubling trick but runs in linearithmic time. Both results are improvements with respect to previous approaches.},
archivePrefix = {arXiv},
arxivId = {arXiv:1001.4475v2},
author = {Bubeck, S{\'{e}}bastien and Munos, R and Stoltz, Gilles and Szepesv{\'{a}}ri, C},
eprint = {arXiv:1001.4475v2},
isbn = {1532-4435},
issn = {15324435},
journal = {Multi-Armed Bandits},
pages = {1--38},
title = {{X-armed bandits}},
url = {http://arxiv.org/abs/1001.4475},
year = {2010}
}

@article{Wang2014,
abstract = {Bayesian optimization is a powerful global optimization technique for expensive black-box functions. One of its shortcomings is that it requires auxiliary optimization of an acquisition function at each iteration. This auxiliary optimization can be costly and very hard to carry out in practice. Moreover, it creates serious theoretical concerns, as most of the convergence results assume that the exact optimum of the acquisition function can be found. In this paper, we introduce a new technique for efficient global optimization that combines Gaussian process confidence bounds and treed simultaneous optimistic optimization to eliminate the need for auxiliary optimization of acquisition functions. The experiments with global optimization benchmarks and a novel application to automatic information extraction demonstrate that the resulting technique is more efficient than the two approaches from which it draws inspiration. Unlike most theoretical analyses of Bayesian optimization with Gaussian processes, our finite-time convergence rate proofs do not require exact optimization of an acquisition function. That is, our approach eliminates the unsatisfactory assumption that a difficult, potentially NP-hard, problem has to be solved in order to obtain vanishing regret rates.},
archivePrefix = {arXiv},
arxivId = {1402.7005},
author = {Wang, Ziyu and Shakibi, Babak and Jin, Lin and de Freitas, Nando},
eprint = {1402.7005},
issn = {15337928},
journal = {AISTATS},
pages = {15},
title = {{Bayesian Multi-Scale Optimistic Optimization}},
url = {http://arxiv.org/abs/1402.7005},
volume = {33},
year = {2014}
}

@article{Snelson2006,
abstract = {We present a new Gaussian process (GP) regression model whose covariance is parameterized by the the locations of M pseudo-input points, which we learn by a gradient based optimization. We take M N, where N is the number of real data points, and hence obtain a sparse regression method which has O(M2N) training cost and O(M2 ) prediction cost per test case. We also find hyperparameters of the covariance function in the same joint optimization. The method can be viewed as a Bayesian regression model with particular input dependent noise. The method turns out to be closely related to several other sparse GP approaches, and we discuss the relation in detail. We finally demonstrate its performance on some large data sets, and make a direct comparison to other sparse GP methods. We show that our method can match full GP performance with small M, i.e. very sparse solutions, and it significantly outperforms other approaches in this regime.},
archivePrefix = {arXiv},
arxivId = {1402.1389},
author = {Snelson, Edward and Ghahramani, Zoubin},
doi = {10.1.1.60.2209},
eprint = {1402.1389},
isbn = {9780262232531},
issn = {1049-5258},
journal = {Advances in Neural Information Processing Systems 18},
pages = {1257--1264},
pmid = {236331100003},
title = {{Sparse Gaussian Processes using Pseudo-inputs}},
url = {http://papers.nips.cc/paper/2857-sparse-gaussian-processes-using-pseudo-inputs.pdf},
year = {2006}
}

@article{Quinonero-candela2005,
abstract = {We provide a new unifying view, including all existing proper probabilistic sparse approximations for Gaussian process regression. Our approach relies on expressing the effective prior which the methods are using. This allows new insights to be gained, and highlights the relationship between existing methods. It also allows for a clear theoretically justified ranking of the closeness of the known approximations to the corresponding full GPs. Finally we point directly to designs of new better sparse approximations, combining the best of the existing strategies, within attractive computational constraints.},
author = {Qui{\~{n}}onero-candela, Joaquin and Rasmussen, Carl Edward and Herbrich, Ralf},
isbn = {1532-4435},
issn = {1533-7928},
journal = {Journal of Machine Learning Research},
keywords = {Bayesian committee,Gaussian process,machine,probabilistic regression,sparse approximation},
pages = {1935--1959},
pmid = {236331100003},
title = {{A unifying view of sparse approximate Gaussian process regression}},
url = {http://jmlr.org/papers/volume6/quinonero-candela05a/quinonero-candela05a.pdf},
volume = {6},
year = {2005}
}

@article{Titsias2009,
abstract = {Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximations that jointly infers the inducing inputs and the kernel hyperparameters by maximizing a lower bound of the true log marginal likelihood. The key property of this formulation is that the inducing inputs are defined to be variational parameters which are selected by minimizing the Kullback-Leibler divergence between the variational distribution and the exact posterior distribution over the latent function values. We apply this technique to regression and we compare it with other approaches in the literature.},
author = {Titsias, Michalis},
issn = {15324435},
journal = {Aistats},
keywords = {Learning/Statistics {\&} Optimisation,Theory {\&} Algorithms},
pages = {567--574},
title = {{Variational Learning of Inducing Variables in Sparse Gaussian Processes}},
url = {http://eprints.pascal-network.org/archive/00006353/},
volume = {5},
year = {2009}
}

@article{Seeger2003,
abstract = {We present a method for the sparse greedy approximation of Bayesian Gaussian process regression, featuring a novel heuristic for very fast forward selection. Our method is essentially as fast as an equivalent one which selects the "support" patterns at random, yet it can outperform random selection on hard curve fitting tasks. More importantly, it leads to a suciently stable approximation of the log marginal likelihood of the training data, which can be optimised to adjust a large number of hyperparameters automatically.},
author = {Seeger, M and Williams, Cki and Lawrence, Nd},
journal = {Workshop on AI and Statistics},
pages = {2003},
title = {{Fast forward selection to speed up sparse Gaussian process regression}},
url = {http://ipg.epfl.ch/{~}seeger/lapmalmainweb/papers/aistats03-final.pdf},
volume = {9},
year = {2003}
}

@article{Lazaro-Gredilla2010,
abstract = {We present a new sparse Gaussian Process (GP) model for regression. The key novel idea is to sparsify the spectral representation of the GP. This leads to a simple, practical algorithm for regression tasks. We compare the achievable trade-offs between predictive accuracy and computational requirements, and show that these are typically superior to existing state-of-the-art sparse approximations. We discuss both the weight space and function space representations, and note that the new construction implies priors over functions which are always stationary, and can approximate any covariance function in this class.},
archivePrefix = {arXiv},
arxivId = {arXiv:1304.6949v1},
author = {L{\'{a}}zaro-Gredilla, M and Quinonero-Candela, J and Rasmussen, C E and Figueiras-Vidal, A R},
doi = {10.1137/10080991X},
eprint = {arXiv:1304.6949v1},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {gp},
pages = {1865--1881},
pmid = {64084887},
title = {{Sparse Spectrum Gaussian Process Regression}},
url = {http://www.jmlr.org/papers/volume11/lazaro-gredilla10a/lazaro-gredilla10a.pdf},
volume = {11},
year = {2010}
}

@inproceedings{Hutter2011,
abstract = {State-of-the-art algorithms for hard computational problems often expose many parameters that can be modified to improve empirical performance. However, manually exploring the resulting combinatorial space of parameter settings is tedious and tends to lead to unsatisfactory outcomes. Recently, automated approaches for solving this algorithm configuration problem have led to substantial improvements in the state of the art for solving various problems. One promising approach constructs explicit regression models to describe the dependence of target algorithm performance on parameter settings; however, this approach has so far been limited to the optimization of few numerical algorithm parameters on single instances. In this paper, we extend this paradigm for the first time to general algorithm configuration problems, allowing many categorical parameters and optimization for sets of instances. We experimentally validate our new algorithm configuration procedure by optimizing a local search and a tree search solver for the propositional satisfiability problem (SAT), as well as the commercial mixed integer programming (MIP) solver CPLEX. In these experiments, our procedure yielded state-of-the-art performance, and in many cases outperformed the previous best configuration approach.},
author = {Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-25566-3_40},
isbn = {9783642255656},
issn = {03029743},
pages = {507--523},
title = {{Sequential model-based optimization for general algorithm configuration}},
volume = {6683 LNCS},
year = {2011}
}

@article{Breiman2001,
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the corre-lation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth Interna-tional conference, * * * , 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
archivePrefix = {arXiv},
arxivId = {http://dx.doi.org/10.1023{\%}2FA{\%}3A1010933404324},
author = {Breiman, Leo},
doi = {10.1023/A:1010933404324},
eprint = {/dx.doi.org/10.1023{\%}2FA{\%}3A1010933404324},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {Classification,Ensemble,Regression},
number = {1},
pages = {5--32},
pmid = {21816105},
primaryClass = {http:},
title = {{Random forests}},
volume = {45},
year = {2001}
}

@article{Desautels2012,
abstract = {Can one parallelize complex exploration– exploitation tradeoffs? As an example, consider the problem of optimal high- throughput experimental design, where we wish to sequentially design batches of experiments in order to simultaneously learn a surrogate function mapping stimulus to response and identify the maximum of the function. We formalize the task as a multi- armed bandit problem, where the unknown payoff function is sampled from a Gaussian process (GP), and instead of a single arm, in each round we pull a batch of several arms in parallel. We develop GP-BUCB, a principled algorithm for choosing batches, based on the GP-UCB algorithm for sequential GP optimization. We prove a surprising result; as compared to the sequential approach, the cumulative regret of the parallel algorithm only increases by a constant factor indepen- dent of the batch size B. Our results provide rigorous theoretical support for exploiting parallelism in Bayesian global optimization. We demonstrate the effectiveness of our approach on two real-world applications. 1.},
archivePrefix = {arXiv},
arxivId = {1206.6402},
author = {Desautels, Thomas and Krause, Andreas and Burdick, Joel},
eprint = {1206.6402},
isbn = {978-1-4503-1285-1},
journal = {Active Learning},
pages = {3873--3923},
title = {{Parallelizing exploration-exploitation tradeoffs with gaussian process bandit optimization}},
url = {http://arxiv.org/abs/1206.6402},
volume = {15},
year = {2012}
}

@article{Coyle2015,
abstract = {—In recent years sports analytics has gotten more and more popular. We propose a model for Rugby data -in particular to model the 2014 Six Nations tournament. We propose a Bayesian hierarchical model to estimate the characteristics that bring a team to lose or win a game, and predict the score of particular matches. This is intended to be a brief introduction to Probabilistic Programming in Python and in particular the powerful library called PyMC3.},
archivePrefix = {arXiv},
arxivId = {1607.0379},
author = {Coyle, Peadar},
eprint = {1607.0379},
journal = {PROC. OF THE 8th EUR. CONF. ON PYTHON IN SCIENCE},
keywords = {Bayesian Statistics,Hierarchical models,Index Terms—MCMC,Probabilistic Programming,PyMC3,Sports Analytics,monte carlo},
title = {{Probabilistic Programming and PyMC3}},
year = {2015}
}

@article{Shah2014,
abstract = {Finding the global minimum of a function is often difficult. We consider efficiently minimizing functions which are computationally expensive to evaluate. A Bayesian approach to the global function optimization problem places a prior distribution on the function and chooses where to evaluate the function based on its posterior distribution given a set of observations. While many recent applications use Gaussian processes as a prior for the objective function, here we show that a Student-t process is an ideal prior for such a problem, as it is also nonparametric, but naturally models heavy tailed behaviour and has a predictive covariance which explicitly depends on observations.},
author = {Shah, Amar and Wilson, Andrew Gordon and Ghahramani, Zoubin},
file = {:home/jose/tpoptimisation.pdf:pdf},
mendeley-groups = {TFM},
number = {2},
pages = {1--5},
title = {{Bayesian Optimization using Student-t Processes}},
year = {2014}
}


@article{Locatelli1997,
abstract = {In this paper Bayesian analysis and Wiener process are used in order to build an algorithm to solve the problem of global optimization. The paper is divided in two main parts. In the first part an already known algorithm is considered: a new (Bayesian) stopping rule is added to it and some results are given, such as an upper bound for the number of iterations under the new stopping rule. In the second part a new algorithm is introduced in which the Bayesian approach is exploited not only in the choice of the Wiener model but also in the estimation of the parameter {\$}\backslashbackslash{\$}sigma{\^{}}2 of the Wiener process, whose value appears to be quite crucial. Some results about this algorithm are also given.},
author = {Locatelli, Marco},
doi = {Doi 10.1023/A:1008294716304},
isbn = {0925-5001},
issn = {0925-5001},
journal = {J. Global Optim.},
keywords = {bayesian analysis,stopping rule,wiener process},
number = {1},
pages = {57--76},
title = {{Bayesian algorithms for one-dimensional global optimization}},
volume = {10},
year = {1997}
}

@article{Ilinskas2002,
author = {Ilinskas, A},
file = {:home/jose/1-s2.0-S0898122102002067-main.pdf:pdf},
keywords = {-global,convergence,optimization,partitioning,simplices,statistical models},
mendeley-groups = {TFM},
number = {02},
title = {{Global Optimization Based on a Statistical Model and Simplicial Partitioning}},
volume = {1221},
year = {2002}
}

@article{Vazquez2010,
abstract = {This paper deals with the convergence of the expected improvement algorithm, a popular global optimization algorithm based on a Gaussian process model of the function to be optimized. The first result is that under some mild hypotheses on the covariance function k of the Gaussian process, the expected improvement algorithm produces a dense sequence of evaluation points in the search domain, when the function to be optimized is in the reproducing kernel Hilbert space generated by k. The second result states that the density property also holds for P-almost all continuous functions, where P is the (prior) probability distribution induced by the Gaussian process. ?? 2010 Elsevier B.V.},
archivePrefix = {arXiv},
arxivId = {0712.3744},
author = {Vazquez, Emmanuel and Bect, Julien},
doi = {10.1016/j.jspi.2010.04.018},
eprint = {0712.3744},
file = {:home/jose/1-s2.0-S0378375810001850-main.pdf:pdf},
isbn = {0378-3758},
issn = {03783758},
journal = {Journal of Statistical Planning and Inference},
keywords = {Bayesian optimization,Computer experiments,Gaussian process,Global optimization,RKHS,Sequential esign},
mendeley-groups = {TFM},
number = {11},
pages = {3088--3095},
publisher = {Elsevier},
title = {{Convergence properties of the expected improvement algorithm with fixed mean and covariance functions}},
url = {http://dx.doi.org/10.1016/j.jspi.2010.04.018},
volume = {140},
year = {2010}
}

@article{Srinivas2009,
abstract = {Many applications require optimizing an unknown, noisy function that is expensive to evaluate. We formalize this task as a multi-armed bandit problem, where the payoff function is either sampled from a Gaussian process (GP) or has low RKHS norm. We resolve the important open problem of deriving regret bounds for this setting, which imply novel convergence rates for GP optimization. We analyze GP-UCB, an intuitive upper-confidence based algorithm, and bound its cumulative regret in terms of maximal information gain, establishing a novel connection between GP optimization and experimental design. Moreover, by bounding the latter in terms of operator spectra, we obtain explicit sublinear regret bounds for many commonly used covariance functions. In some important cases, our bounds have surprisingly weak dependence on the dimensionality. In our experiments on real sensor data, GP-UCB compares favorably with other heuristical GP optimization approaches.},
archivePrefix = {arXiv},
arxivId = {0912.3995},
author = {Srinivas, Niranjan and Krause, Andreas and Kakade, Sham M. and Seeger, Matthias},
doi = {10.1109/TIT.2011.2182033},
eprint = {0912.3995},
file = {:home/jose/bandit{\_}GP{\_}icml.pdf:pdf},
isbn = {9781605589077},
issn = {00189448},
mendeley-groups = {TFM},
title = {{Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design}},
url = {http://arxiv.org/abs/0912.3995{\%}0Ahttp://dx.doi.org/10.1109/TIT.2011.2182033},
year = {2009}
}




